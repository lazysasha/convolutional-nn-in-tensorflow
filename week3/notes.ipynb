{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Week 3: Transfer Learning\n",
    "TL allows re-using existing model and learnt features just by downloading an open source model and use those params as a starting point, and train your model a bit more on a smaller dataset.\n",
    "\n",
    "## Understanding TL concepts\n",
    "Problem of previous week DS is that training data was very small, and there is only so many common features that can be extracted (even with image augmentation).\n",
    "Instead of building a model from scratch, we can use an existing model that is trained on far more data and use the features that that model learned. You can use the features that were learned from large datasets that you may not have access to.\n",
    "You can choose to re-train some of the lower layers, because they might be too specialized for the images at hand.\n",
    "\n",
    "In the next few videos you'll be using this [notebook](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/Course%202%20-%20Part%206%20-%20Lesson%203%20-%20Notebook.ipynb) to explore transfer learning.\n",
    "For more on how to freeze/lock layers (`layer.trainable = false`), explore the documentation, which includes an example using MobileNet architecture [Transfer learning and fine-tuning](https://www.tensorflow.org/tutorials/images/transfer_learning)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Coding transfer learning from the inception mode\n",
    "import os\n",
    "\n",
    "# We will use Keras Layer API to peek at the layers and understand which ones we want to use and which ones to retrain\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5' # pretrained weights of inception Neural Network\n",
    "pre_trained_model = InceptionV3(input_shape=(150, 150, 3),\n",
    "                                include_top=False, # do not use built-in weights, ignore fully connected layer at the top and get straight to convolutions\n",
    "                                weights=None)\n",
    "pre_trained_model.load_weights(local_weights_file)\n",
    "\n",
    "# Iterate through layers and lock them, saying that they are not gonna be trainable\n",
    "for layer in pre_trained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "pre_trained_model.summary() # lost of data!\n",
    "\n",
    "# Coding your own model with transferred features\n",
    "last_layer = pre_trained_model.get_layer('mixed7') # all of the layers have name, so we can get any layer by name\n",
    "last_output = last_layer.output # take the output of the layer\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "# Define our new model, taking `last_output` layer from inception model\n",
    "x = layers.Flatten()(last_output)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "x = layers.Dropout(0.2)(x) # drop out 20% of the neurons\n",
    "\n",
    "model = Model(pre_trained_model.input, x)\n",
    "model.compile(optimizer=RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# augment images with image generator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255.,\n",
    "                                   rotation_range = 40,\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "# get training data from generator:\n",
    "base_dir = '/tmp/cats_and_dogs_filtered'\n",
    "train_dir = os.path.join( base_dir, 'train')\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=20,\n",
    "    class_mode='binary',\n",
    "    target_size=(150,150)\n",
    ")\n",
    "\n",
    "# train the network:\n",
    "validation_generator = None #TODO: set it up\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=100,\n",
    "    validation_steps=50,\n",
    "    verbose=2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using dropouts\n",
    "Dropout is a regularization technique that prevents over fitting\n",
    "The idea behind Dropouts is that they remove a random number of neurons in your neural network.\n",
    "This works very well for two reasons: The first is that neighboring neurons often end up with similar weights, which can lead to overfitting, so dropping some out at random can remove this.\n",
    "The second is that often a neuron can over-weigh the input from a neuron in the previous layer, and can over specialize as a result.\n",
    "Thus, dropping out can break the neural network out of this potential bad habit!\n",
    "\n",
    "Check out Andrew's terrific video explaining dropouts [here](https://www.youtube.com/watch?v=ARq74QuavAo)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}